{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b73d5-014b-4542-ba39-cd99f55029f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import uuid\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.transforms import Join, SelectFields\n",
    "from pyspark.sql.functions import col\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "import scispacy\n",
    "import spacy\n",
    "import random\n",
    "nlp = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from awsglue.transforms import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(\"data-eng-updated\")\n",
    "\n",
    "print('Updated file locally')\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon') # download the pre-trained model\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# create an instance of the pre-trained sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentence = \"Amazing movie, I really enjoyed it\"\n",
    "\n",
    "# use the sentiment analyzer to get a sentiment score for the sentence\n",
    "sentiment = sia.polarity_scores(sentence)\n",
    "\n",
    "# print the sentiment score\n",
    "print(sentiment[\"pos\"])\n",
    "customer_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database='data-eng-s3-demo-db', table_name='test_customers',transformation_ctx = \"datasource1\" ,additional_options = {\"jobBookmarkKeys\":[\"id\"],\"jobBookmarkKeysSortOrder\":\"asc\"})\n",
    "customer_data_frame = customer_dynamic_frame.toDF()\n",
    "customer_data_frame.printSchema()\n",
    "wearable_details = glueContext.create_dynamic_frame.from_catalog(database='data-eng-s3-demo-db', table_name='test_wearables',transformation_ctx = \"datasource0\", additional_options = {\"jobBookmarkKeys\":[\"user_id\"],\"jobBookmarkKeysSortOrder\":\"asc\"})\n",
    "wearable_df = wearable_details.toDF()\n",
    "wearable_df.show()\n",
    "providers_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database='data-eng-s3-demo-db', table_name='provider',transformation_ctx = \"datasource0\" ,additional_options = {\"jobBookmarkKeys\":[\"id\"],\"jobBookmarkKeysSortOrder\":\"asc\"})\n",
    "providers_data_frame = providers_dynamic_frame.toDF()\n",
    "providers_dynamic_frame.printSchema()\n",
    "# Rename device_id to device_model\n",
    "wearable_df = wearable_df.withColumnRenamed('device_id','device_model')\n",
    "wearable_df.persist()\n",
    "# Create distincnt device table\n",
    "distinct_device_df = wearable_df.select('device_model','brand').distinct()\n",
    "# # Define a UDF to generate UUIDs\n",
    "generate_uuid = udf(lambda: str(uuid.uuid4().hex), StringType())\n",
    "\n",
    "# # Add a new column with a unique UUID for each row\n",
    "# distinct_device_df = distinct_device_df.withColumn(\"device_id\", generate_uuid())\n",
    "# distinct_device_df.persist()\n",
    "distinct_device_df.show(100)\n",
    "# joined_devices_and_wearables = wearable_df.join(distinct_device_df, ['device_model', 'brand'], 'left')\n",
    "watch_facts_table = wearable_df.select(\n",
    "    'user_id',\n",
    "    'device_model',\n",
    "    'date',\n",
    "    'steps', \n",
    "    'heart_rate', \n",
    "    'calories_burned', \n",
    "    'sleep_hours', \n",
    "    'active_minutes', \n",
    "    'diastolic_bp', \n",
    "    'body_temp', \n",
    "    'oxygen_saturation', \n",
    "    'respiratory_rate',\n",
    ")\n",
    "user_table = customer_data_frame.select(\n",
    "    'id',\n",
    "    'age',\n",
    "    'gender',\n",
    "    'bmi',\n",
    "    'smoking',\n",
    "    'alcohol',\n",
    "    'family_history',\n",
    "    'pre_existing_conditions',\n",
    "    'medications',\n",
    "    'hospitalization',\n",
    "    'blood_sugar',\n",
    "    'cholesterol',\n",
    "    'blood_pressure',\n",
    "    'vitamin_deficiencies',\n",
    "    'mental_health',\n",
    "    \n",
    "    'occupation',\n",
    "    'work_environment',\n",
    "    'travel_history',\n",
    "    'geographic_location',\n",
    "    'environmental_exposure',\n",
    "    'exercise_frequency',\n",
    "    'dietary_habits',\n",
    "    \n",
    "    'marital_status',\n",
    "    'education_level',\n",
    "    'income_level',\n",
    "    'employment_status',\n",
    "    'dental_health',\n",
    "    'allergies_sensitivities',\n",
    "    'health_plan_type',\n",
    "    'deductible'\n",
    ").distinct()\n",
    "renewal_history_facts_table = customer_data_frame.select(\n",
    "  'id',\n",
    "  'year',\n",
    "  'plan_type',\n",
    "  'cancellation_reason',\n",
    "  'premium_cost',\n",
    "  'health_plan_type',\n",
    "  'deductible'\n",
    ")\n",
    "\n",
    "renewal_history_facts_table = renewal_history_facts_table.withColumnRenamed('id','user_id')\n",
    "\n",
    "# Add a new column with a unique UUID for each row\n",
    "renewal_history_facts_table = renewal_history_facts_table.withColumn(\"id\", generate_uuid())\n",
    "renewal_history_facts_table.persist()\n",
    "def getCurrentTime():\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    return current_time\n",
    "def uploadToRedshift(df, tableName):\n",
    "    try:\n",
    "        df.toDF().write.format(\"jdbc\").\\\n",
    "        option(\"url\", \"jdbc:redshift://health-space.511522223657.ap-south-1.redshift-serverless.amazonaws.com:5439/dev\").\\\n",
    "        option(\"dbtable\", f\"public.{tableName}\").\\\n",
    "        option(\"user\", \"admin\").\\\n",
    "        option(\"password\", \"RedshiftPassword99\").\\\n",
    "        mode('append').save()\n",
    "        print(f\"Successfully created {tableName} table to Redshift\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating {tableName} table to Redshift:\", str(e))\n",
    "        raise e\n",
    "def insertToRedshift(df,tableName):\n",
    "    destinationTable = f\"public.{tableName}\"\n",
    "    glueContext.write_dynamic_frame.from_jdbc_conf(\n",
    "        frame = df,\n",
    "        catalog_connection=\"redshift-new-conn-data-eng\",\n",
    "        connection_options = {\n",
    "            \"database\":\"dev\",\n",
    "            \"dbtable\":destinationTable,\n",
    "            \n",
    "        },\n",
    "        redshift_tmp_dir=\"s3://data-eng-s3-wednesday/tmp/tmpdir-r/\",\n",
    "        transformation_ctx=\"upsert_to_redshift\"\n",
    "    )\n",
    "\n",
    "def upsertToRedshift2(df,tableName,pk):\n",
    "    destinationTable = f\"public.{tableName}\"\n",
    "    destination = f\"dev.{destinationTable}\"\n",
    "    staging = f\"dev.{destinationTable}_staging\"\n",
    "    stg = f\"{destinationTable}_staging\"\n",
    "    \n",
    "    fields = df.toDF().columns\n",
    "    print(fields)\n",
    "\n",
    "    \n",
    "    \n",
    "    postActions = f\"\"\"\n",
    "    DELETE FROM {destination} USING {staging} AS S WHERE {destinationTable}.{pk} = S.{pk};\n",
    "    INSERT INTO {destination} SELECT * FROM {staging};DROP TABLE IF EXISTS {staging}\"\"\"\n",
    "    glueContext.write_dynamic_frame.from_jdbc_conf(\n",
    "        frame = df,\n",
    "        catalog_connection=\"redshift-new-conn-data-eng\",\n",
    "        connection_options = {\n",
    "            \"database\":\"dev\",\n",
    "            \"dbtable\":stg,\n",
    "            \"postActions\":postActions,\n",
    "            \n",
    "        },\n",
    "        redshift_tmp_dir=\"s3://data-eng-s3-wednesday/tmp/tmpdir-r/\",\n",
    "        transformation_ctx=\"upsert_to_redshift\"\n",
    "    )\n",
    "        \n",
    "    \n",
    "    \n",
    "def upsertToRedshift2Multi(df,tableName,pk,sk):\n",
    "    destinationTable = f\"public.{tableName}\"\n",
    "    destination = f\"dev.{destinationTable}\"\n",
    "    staging = f\"dev.{destinationTable}_staging\"\n",
    "    stg = f\"{destinationTable}_staging\"\n",
    "    \n",
    "    fields = df.toDF().columns\n",
    "    print(fields)\n",
    "\n",
    "    \n",
    "    \n",
    "    postActions = f\"\"\"\n",
    "    DELETE FROM {destination} USING {staging} AS S WHERE {destinationTable}.{pk} = S.{pk} and {destinationTable}.{sk} = S.{sk};\n",
    "    INSERT INTO {destination} SELECT * FROM {staging};DROP TABLE IF EXISTS {staging}\"\"\"\n",
    "    glueContext.write_dynamic_frame.from_jdbc_conf(\n",
    "        frame = df,\n",
    "        catalog_connection=\"redshift-new-conn-data-eng\",\n",
    "        connection_options = {\n",
    "            \"database\":\"dev\",\n",
    "            \"dbtable\":stg,\n",
    "            \"postActions\":postActions,\n",
    "            \n",
    "        },\n",
    "        redshift_tmp_dir=\"s3://data-eng-s3-wednesday/tmp/tmpdir-r/\",\n",
    "        transformation_ctx=\"upsert_to_redshift\"\n",
    "    )\n",
    "        \n",
    "empty_device_frame = DynamicFrame.fromDF(distinct_device_df.select('*').limit(0),glueContext, \"watch_facts_data_frame\")\n",
    "uploadToRedshift(empty_device_frame,'devices')\n",
    "\n",
    "\n",
    "print(f\"Start Time: {getCurrentTime()}\")\n",
    "distinct_device_dy_frame = DynamicFrame.fromDF(distinct_device_df.select('*'), glueContext, \"watch_facts_data_frame\")\n",
    "\n",
    "upsertToRedshift2(distinct_device_dy_frame, 'devices','device_model')\n",
    "print(f\"End Time: {getCurrentTime()}\")\n",
    "watch_facts_table.show()\n",
    "empty_device_frame = DynamicFrame.fromDF(watch_facts_table.select('*').limit(0),glueContext, \"watch_facts_data_frame\")\n",
    "uploadToRedshift(empty_device_frame,'watch_facts')\n",
    "\n",
    "print(f\"Start Time: {getCurrentTime()}\")\n",
    "watch_facts_dy_frame = DynamicFrame.fromDF(watch_facts_table.select('*'), glueContext, \"watch_facts_data_frame\")\n",
    "print(f\"Start Time 2: {getCurrentTime()}\")\n",
    "upsertToRedshift2Multi(watch_facts_dy_frame,'watch_facts','user_id','date')\n",
    "print(f\"End Time: {getCurrentTime()}\")\n",
    "empty_users_frame = DynamicFrame.fromDF(user_table.select('*').limit(0),glueContext, \"users_data_frame\")\n",
    "uploadToRedshift(empty_users_frame,'users')\n",
    "user_table.printSchema()\n",
    "print(f\"Start Time: {getCurrentTime()}\")\n",
    "users_dy_frame = DynamicFrame.fromDF(user_table.select('*'), glueContext, \"users_data_frame\")\n",
    "upsertToRedshift2(users_dy_frame, 'users','id')\n",
    "\n",
    "print(f\"End Time: {getCurrentTime()}\")\n",
    "renewal_empty_frame = DynamicFrame.fromDF(renewal_history_facts_table.select('*').limit(0),glueContext, \"users_data_frame\")\n",
    "uploadToRedshift(renewal_empty_frame, 'renewal_history')\n",
    "print(f\"Start Time: {getCurrentTime()}\")\n",
    "renewal_history_dy_frame = DynamicFrame.fromDF(renewal_history_facts_table, glueContext, \"renewal_history_data_frame\")\n",
    "upsertToRedshift2(renewal_history_dy_frame, 'renewal_history','id')\n",
    "print(f\"End Time: {getCurrentTime()}\")\n",
    "provider_empty_frame = DynamicFrame.fromDF(providers_data_frame.select('*').limit(0),glueContext, \"providers_data_frame\")\n",
    "uploadToRedshift(provider_empty_frame, 'provider')\n",
    "print(f\"Start Time: {getCurrentTime()}\")\n",
    "provider_dy_frame = DynamicFrame.fromDF(providers_data_frame, glueContext, \"providers_data_frame\")\n",
    "upsertToRedshift2(provider_dy_frame, 'provider','id')\n",
    "print(f\"End Time: {getCurrentTime()}\")\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'data-eng-s3-wednesday'\n",
    "prefix = 'notes/admission_notes/'  # Include trailing slash\n",
    "s3_bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "admission_texts = []\n",
    "\n",
    "for obj in s3_bucket.objects.filter(Prefix=prefix):\n",
    "    if obj.key.endswith('.txt'):\n",
    "        s3_object = s3.Object(bucket_name, obj.key)\n",
    "        file_content = s3_object.get()['Body'].read().decode('utf-8')\n",
    "        individual_texts = file_content.split('\\n')  # Or any other delimiter that separates texts\n",
    "        individual_texts = [x for x in individual_texts if x != \"\"]\n",
    "        admission_texts.extend(individual_texts)\n",
    "    \n",
    "user_spark = DynamicFrame.fromDF(user_table.select('*'),glueContext, \"users_data_frame\").toDF()\n",
    "provider_spark = DynamicFrame.fromDF(providers_data_frame.select('*'),glueContext,\"providers_data_frame\").toDF()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "u = user_spark.select('id')\n",
    "getNewAdmissionText = udf(lambda: admission_texts[random.randint(0, len(admission_texts)-1)])\n",
    "u = u.withColumnRenamed('id','user_id')\n",
    "providerId = udf(lambda: random.randint(1, 2000))\n",
    "notesType = udf(lambda: \"admission_notes\" )\n",
    "def getNewAdmissionTextSentiment(s):\n",
    "    a = sia.polarity_scores(s)\n",
    "    return a[\"compound\"]+a[\"neu\"]\n",
    "getNewAdmissionTextSentiment_udf = udf(getNewAdmissionTextSentiment, DoubleType())\n",
    "u = u.withColumn('notes', getNewAdmissionText())\n",
    "u = u.withColumn('provider_id', providerId())\n",
    "u = u.withColumn('notes_pos_sentiment', getNewAdmissionTextSentiment_udf(col('notes')))\n",
    "u = u.withColumn('id', generate_uuid())\n",
    "u = u.withColumn('notes_type',notesType())\n",
    "u.persist()\n",
    "u.show()\n",
    "\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'data-eng-s3-wednesday'\n",
    "prefix = 'notes/admission_follow_up/'  # Include trailing slash\n",
    "s3_bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "follow_up_texts = []\n",
    "\n",
    "for obj in s3_bucket.objects.filter(Prefix=prefix):\n",
    "    if obj.key.endswith('.txt'):\n",
    "        s3_object = s3.Object(bucket_name, obj.key)\n",
    "        file_content = s3_object.get()['Body'].read().decode('utf-8')\n",
    "        individual_texts = file_content.split('----------------------------------------------------------------------------------------------------')  # Or any other delimiter that separates texts\n",
    "        individual_texts = [x for x in individual_texts if x != \"\"]\n",
    "        follow_up_texts.extend(individual_texts)\n",
    "\n",
    "t = user_spark.select('id')\n",
    "getNewFollowUpText = udf(lambda: follow_up_texts[random.randint(0, len(follow_up_texts)-1)])\n",
    "t = t.withColumnRenamed('id','user_id')\n",
    "\n",
    "notesTypeFollowUp = udf(lambda: \"follow_up_notes\" )\n",
    "\n",
    "    \n",
    "getNewAdmissionTextSentiment_udf = udf(getNewAdmissionTextSentiment, DoubleType())\n",
    "t = t.withColumn('notes', getNewFollowUpText())\n",
    "t = t.withColumn('provider_id', providerId())\n",
    "t = t.withColumn('notes_pos_sentiment', getNewAdmissionTextSentiment_udf(col('notes')))\n",
    "t = t.withColumn('id', generate_uuid())\n",
    "t = t.withColumn('notes_type',notesType())\n",
    "t.persist()\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'data-eng-s3-wednesday'\n",
    "prefix = 'notes/discharge_notes/'  # Include trailing slash\n",
    "s3_bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "discharge_texts = []\n",
    "\n",
    "for obj in s3_bucket.objects.filter(Prefix=prefix):\n",
    "    if obj.key.endswith('.txt'):\n",
    "        s3_object = s3.Object(bucket_name, obj.key)\n",
    "        file_content = s3_object.get()['Body'].read().decode('utf-8')\n",
    "        individual_texts = file_content.split('-------------------------------------------------------')  # Or any other delimiter that separates texts\n",
    "        individual_texts = [x for x in individual_texts if x != \"\"]\n",
    "        discharge_texts.extend(individual_texts)\n",
    "v = user_spark.select('id')\n",
    "getNewDischargeText = udf(lambda: follow_up_texts[random.randint(0, len(discharge_texts)-1)])\n",
    "v = v.withColumnRenamed('id','user_id')\n",
    "\n",
    "notesTypeDischarge = udf(lambda: \"discharge_notes\" )\n",
    "\n",
    "    \n",
    "getNewAdmissionTextSentiment_udf = udf(getNewAdmissionTextSentiment, DoubleType())\n",
    "v = v.withColumn('notes', getNewDischargeText())\n",
    "v = v.withColumn('provider_id', providerId())\n",
    "v = v.withColumn('notes_pos_sentiment', getNewAdmissionTextSentiment_udf(col('notes')))\n",
    "v = v.withColumn('id', generate_uuid())\n",
    "v = v.withColumn('notes_type',notesTypeDischarge())\n",
    "v = v.withColumn('notes', regexp_replace('notes', '\\n', ''))\n",
    "v.select('*').show()\n",
    "v.persist()\n",
    "all_notes_df = u.union(t).union(v)\n",
    "all_notes_df.persist()\n",
    "all_notes_df.show()\n",
    "# notes_df.where(notes_df.notes_type == 'discharge_notes')\n",
    "\n",
    "# notes_df.persist()\n",
    "# notes_df.count()\n",
    "\n",
    "# v.show()\n",
    "\n",
    "notes_empty_frame = DynamicFrame.fromDF(all_notes_df,glueContext, \"doctor_notes_data_frame\")\n",
    "insertToRedshift(notes_empty_frame, 'doctor_notes')\n",
    "# print(f\"Start Time: {getCurrentTime()}\")\n",
    "# renewal_history_dy_frame = DynamicFrame.fromDF(renewal_history_facts_table, glueContext, \"renewal_history_data_frame\")\n",
    "# upsertToRedshift2(renewal_history_dy_frame, 'renewal_history','id')\n",
    "# print(f\"End Time: {getCurrentTime()}\")\n",
    "print('Done with script, updated file name')\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d0ab24",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
